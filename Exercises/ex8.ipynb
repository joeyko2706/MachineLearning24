{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bTh97SOF7BG"
      },
      "source": [
        "# Exercise 8 - Recurrent Neural networks\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Zv_6jnmF7BH"
      },
      "source": [
        "In this exercise we want to use recurrent neural networks in order to classify the IMDB dataset.\n",
        "\n",
        "This exercise is based on https://github.com/leriomaggio/deep-learning-keras-tensorflow and https://github.com/keras-team/keras/blob/master/examples/imdb_lstm.py and https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq9gtOCMF7BL"
      },
      "source": [
        "<img src=\"https://austingwalters.com/wp-content/uploads/2019/01/rnn-multihidden.png\" width=\"40%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51LNl4zYF7BM"
      },
      "source": [
        "A recurrent neural network (RNN) is a class of artificial neural network where connections between units form a directed cycle. This creates an internal state of the network which allows it to exhibit dynamic temporal behavior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "BrvwcNq6F7BM"
      },
      "source": [
        "```python\n",
        "tensorflow.keras.layers.SimpleRNN(units, activation='tanh', use_bias=True,\n",
        "                                 kernel_initializer='glorot_uniform',\n",
        "                                 recurrent_initializer='orthogonal',\n",
        "                                 bias_initializer='zeros',\n",
        "                                 kernel_regularizer=None,\n",
        "                                 recurrent_regularizer=None,\n",
        "                                 bias_regularizer=None,\n",
        "                                 activity_regularizer=None,\n",
        "                                 kernel_constraint=None, recurrent_constraint=None,\n",
        "                                 bias_constraint=None, dropout=0.0, recurrent_dropout=0.0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8lJg6dFF7BN"
      },
      "source": [
        "#### Arguments:\n",
        "\n",
        "<ul>\n",
        "<li><strong>units</strong>: Positive integer, dimensionality of the output space.</li>\n",
        "<li><strong>activation</strong>: Activation function to use\n",
        "    (see <a href=\"http://keras.io/activations/\">activations</a>).\n",
        "    If you pass None, no activation is applied\n",
        "    (ie. \"linear\" activation: <code>a(x) = x</code>).</li>\n",
        "<li><strong>use_bias</strong>: Boolean, whether the layer uses a bias vector.</li>\n",
        "<li><strong>kernel_initializer</strong>: Initializer for the <code>kernel</code> weights matrix,\n",
        "    used for the linear transformation of the inputs.\n",
        "    (see <a href=\"https://keras.io/initializers/\">initializers</a>).</li>\n",
        "<li><strong>recurrent_initializer</strong>: Initializer for the <code>recurrent_kernel</code>\n",
        "    weights matrix,\n",
        "    used for the linear transformation of the recurrent state.\n",
        "    (see <a href=\"https://keras.io/initializers/\">initializers</a>).</li>\n",
        "<li><strong>bias_initializer</strong>: Initializer for the bias vector\n",
        "    (see <a href=\"https://keras.io/initializers/\">initializers</a>).</li>\n",
        "<li><strong>kernel_regularizer</strong>: Regularizer function applied to\n",
        "    the <code>kernel</code> weights matrix\n",
        "    (see <a href=\"https://keras.io/regularizers/\">regularizer</a>).</li>\n",
        "<li><strong>recurrent_regularizer</strong>: Regularizer function applied to\n",
        "    the <code>recurrent_kernel</code> weights matrix\n",
        "    (see <a href=\"https://keras.io/regularizers/\">regularizer</a>).</li>\n",
        "<li><strong>bias_regularizer</strong>: Regularizer function applied to the bias vector\n",
        "    (see <a href=\"https://keras.io/regularizers/\">regularizer</a>).</li>\n",
        "<li><strong>activity_regularizer</strong>: Regularizer function applied to\n",
        "    the output of the layer (its \"activation\").\n",
        "    (see <a href=\"https://keras.io/regularizers/\">regularizer</a>).</li>\n",
        "<li><strong>kernel_constraint</strong>: Constraint function applied to\n",
        "    the <code>kernel</code> weights matrix\n",
        "    (see <a href=\"https://keras.io/constraints/\">constraints</a>).</li>\n",
        "<li><strong>recurrent_constraint</strong>: Constraint function applied to\n",
        "    the <code>recurrent_kernel</code> weights matrix\n",
        "    (see <a href=\"https://keras.io/constraints/\">constraints</a>).</li>\n",
        "<li><strong>bias_constraint</strong>: Constraint function applied to the bias vector\n",
        "    (see <a href=\"https://keras.io/constraints/\">constraints</a>).</li>\n",
        "<li><strong>dropout</strong>: Float between 0 and 1.\n",
        "    Fraction of the units to drop for\n",
        "    the linear transformation of the inputs.</li>\n",
        "<li><strong>recurrent_dropout</strong>: Float between 0 and 1.\n",
        "    Fraction of the units to drop for\n",
        "    the linear transformation of the recurrent state.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdgJYl84F7BO"
      },
      "source": [
        "#### Backprop Through time  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InQx3bfZF7BP"
      },
      "source": [
        "Contrary to feed-forward neural networks, the RNN is characterized by the ability of encoding longer past information, thus very suitable for sequential models. The BPTT extends the ordinary BP algorithm to suit the recurrent neural\n",
        "architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7RdFFVNF7BQ",
        "scrolled": true
      },
      "source": [
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ee/Unfold_through_time.png\" width=\"45%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXYOBWmbF7BR"
      },
      "source": [
        "**Reference**: [Backpropagation through Time](http://ir.hit.edu.cn/~jguo/docs/notes/bptt.pdf)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IK5QmCAAF7BS"
      },
      "source": [
        "## IMDB sentiment classification task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v3yzby8F7BT"
      },
      "source": [
        "The problem that we will use to demonstrate sequence learning in this tutorial is the IMDB movie review sentiment classification problem. Each movie review is a variable sequence of words and the sentiment of each movie review must be classified.\n",
        "\n",
        "The Large Movie Review Dataset (often referred to as the IMDB dataset) contains 25,000 highly-polar movie reviews (good or bad) for training and the same amount again for testing. The problem is to determine whether a given movie review has a positive or negative sentiment.\n",
        "http://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "The data was collected by Stanford researchers and was used in a 2011 paper (http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf) where a split of 50-50 of the data was used for training and test. An accuracy of 88.89% was achieved.\n",
        "\n",
        "Keras provides access to the IMDB dataset built-in. The imdb.load_data() function allows you to load the dataset in a format that is ready for use in neural network and deep learning models.\n",
        "\n",
        "The words have been replaced by integers that indicate the ordered frequency of each word in the dataset. The sentences in each review are therefore comprised of a sequence of integers.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEqnQqBcF7BU"
      },
      "source": [
        "### Word Embedding\n",
        "We will map each movie review into a real vector domain, a popular technique when working with text called word embedding. This is a technique where words are encoded as real-valued vectors in a high dimensional space, where the similarity between words in terms of meaning translates to closeness in the vector space.\n",
        "\n",
        "Keras provides a convenient way to convert positive integer representations of words into a word embedding by an Embedding layer.\n",
        "\n",
        "We will map each word onto a 32 length real valued vector. We will also limit the total number of words that we are interested in modeling to the 10000 most frequent words, and zero out the rest. Finally, the sequence length (number of words) in each review varies, so we will constrain each review to be 500 words, truncating long reviews and pad the shorter reviews with zero values.\n",
        "\n",
        "Now that we have defined our problem and how the data will be prepared and modeled, we are ready to develop an LSTM model to classify the sentiment of movie reviews."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r21wPDLkF7BU"
      },
      "source": [
        "### Data Preparation - IMDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOOctrhZpgrO"
      },
      "source": [
        "As usual we will load tensorflow 2 and make sure we use python 3:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "id": "Dw8bkURNpdYe",
        "outputId": "667823d1-2318-4676-b7e1-4b1a6825a2e9"
      },
      "source": [
        "#Check if colab is running\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "  %tensorflow_version 2.x\n",
        "\n",
        "#import TF\n",
        "import tensorflow as tf\n",
        "from platform import python_version\n",
        "print(\"Tensorflow version\", tf.__version__)\n",
        "print(\"Python version =\",python_version())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python version = 3.6.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBt0kdy69ydJ"
      },
      "source": [
        "Importing everything we need..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSx1ErCYF7BV"
      },
      "source": [
        "import numpy\n",
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import LSTM, SimpleRNN\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "numpy.random.seed(42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yr7EfwuaF7Bb"
      },
      "source": [
        "We need to load the IMDB dataset. We are constraining the dataset to the top 10,000 most commonly used words. We also split the dataset into train (70%) and validation (30%) sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66
        },
        "id": "DAIVQj1KF7Bc",
        "outputId": "ea8ddf82-9e56-4f97-f290-d8d8fc09c313"
      },
      "source": [
        "# load the dataset but only keep the top n words, zero the rest\n",
        "top_words = 10000\n",
        "print(\"Loading data...\")\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)\n",
        "\n",
        "#Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=42, stratify=y_train)\n",
        "\n",
        "# Map for readable classnames\n",
        "class_names = [\"Negative\", \"Positive\"]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMWF8dt--Z4f"
      },
      "source": [
        "Let's inspect the data a little bit, how many sequences do we have and how do they look like?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7nUx92O-JCS"
      },
      "source": [
        "print(len(X_test), 'test sequences')\n",
        "print(len(X_train), 'train sequences')\n",
        "print(len(X_val), 'validation sequences')\n",
        "print('Example: ',X_train[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naliGcae-0fw"
      },
      "source": [
        "#### Create map for converting IMDB dataset to readable reviews\n",
        "\n",
        "Apparantely, reviews in the IMDB dataset have been encoded as a sequence of integers. Luckily the dataset also contains an index for converting the reviews back into human readable form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "id": "SAcOiCVp_EQi",
        "outputId": "c3911c75-c9db-4ea8-ced1-4e9f8e8257a1"
      },
      "source": [
        "# Get the word index from the dataset\n",
        "word_index = tf.keras.datasets.imdb.get_word_index()\n",
        "\n",
        "# Ensure that \"special\" words are mapped into human readable terms\n",
        "word_index = {k:(v+3) for k,v in word_index.items()}\n",
        "word_index[\"<PAD>\"] = 0\n",
        "word_index[\"<START>\"] = 1\n",
        "word_index[\"<UNKNOWN>\"] = 2\n",
        "word_index[\"<UNUSED>\"] = 3\n",
        "\n",
        "# Perform reverse word lookup and make it callable\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "def decode_review(text):\n",
        "    return ' '.join([reverse_word_index.get(i, '?') for i in text])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWyhEOKw_MGh"
      },
      "source": [
        "Let's have a closer look at our data. How many words do our reviews contain? And how does  the review look like in human readable form?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjG39CjQABni"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Concatonate test and training datasets\n",
        "allreviews = np.concatenate((X_train, X_test), axis=0)\n",
        "\n",
        "# Review lengths across test and training whole datasets\n",
        "print(\"Maximum review length: {}\".format(len(max((allreviews), key=len))))\n",
        "print(\"Minimum review length: {}\".format(len(min((allreviews), key=len))))\n",
        "result = [len(x) for x in allreviews]\n",
        "print(\"Mean review length: {}\".format(np.mean(result)))\n",
        "\n",
        "# Print a review and it's class as stored in the dataset. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Machine readable Review\")\n",
        "print(\"  Review Text: \" + str(X_train[1]))\n",
        "print(\"  Review Sentiment: \" + str(y_train[1]))\n",
        "\n",
        "# Print a review and it's class in human readable format. Replace the number\n",
        "# to select a different review.\n",
        "print(\"\")\n",
        "print(\"Human Readable Review\")\n",
        "print(\"  Review Text: \" + decode_review(X_train[1]))\n",
        "print(\"  Review Sentiment: \" + class_names[y_train[1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGiDHBQeF7Bk"
      },
      "source": [
        "### Pre-processing Data\n",
        "\n",
        "We need to make sure that our reviews are of a uniform length, which is needed for the LSTM model. Some reviews will need to be truncated, while others need to be padded. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehEspqpqF7Bl"
      },
      "source": [
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
        "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "X_val = sequence.pad_sequences(X_val, maxlen=max_review_length)\n",
        "\n",
        "# Check the size of our datasets. Review data for both test and training should\n",
        "# contain 25000 reviews of 500 integers. Class data should contain 25000 values,\n",
        "# one for each review. Class values are 0 or 1, indicating a negative\n",
        "# or positive review.\n",
        "print(\"Shape Training Review Data: \" + str(X_train.shape))\n",
        "print(\"Shape Training Class Data: \" + str(y_train.shape))\n",
        "print(\"Shape Test Review Data: \" + str(X_test.shape))\n",
        "print(\"Shape Test Class Data: \" + str(y_test.shape))\n",
        "\n",
        "# Note padding is added to start of review, not the end\n",
        "print(\"\")\n",
        "print(\"Machine readable Review Text (post padding):\\n \" + str(X_train[1]))\n",
        "print(\"\")\n",
        "print(\"Human Readable Review Text (post padding):\\n \" + decode_review(X_train[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IApLi2pdF7Bp"
      },
      "source": [
        "## A simple RNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgKP34Z5F7Bq",
        "scrolled": true
      },
      "source": [
        "print('Build model...')\n",
        "# create the model\n",
        "embedding_vecor_length = 32\n",
        "model = Sequential()\n",
        "\n",
        "# The Embedding Layer provides a spatial mapping (or Word Embedding) of all the\n",
        "# individual words in our training set. Words close to one another share context\n",
        "# and or meaning. This spatial mapping is learning during the training process.\n",
        "model.add(\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim = top_words, # The size of our vocabulary\n",
        "        output_dim = embedding_vecor_length, # Dimensions to which each words shall be mapped\n",
        "        input_length = max_review_length # Length of input sequences\n",
        "    )\n",
        ")\n",
        "#Dropout for regularization\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "#We are using the simplest RNN model for now with 128 units\n",
        "model.add(SimpleRNN(128))\n",
        "\n",
        "#A second dropout layer\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "#Connect the output to a single dense output unit\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWMGUmgbF7Bv"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2ke5dBKMIpy"
      },
      "source": [
        "Let's train the model, training 20 epochs with a relatively large batch size of 1024 will take already around 3-5 minutes on a GPU...time to get a coffee..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xMW9Ix0OF7Bw",
        "scrolled": true
      },
      "source": [
        "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFFzjFvdF7B0"
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "def plot_history(network_history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(network_history.history['loss'])\n",
        "    plt.plot(network_history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(network_history.history['accuracy'])\n",
        "    plt.plot(network_history.history['val_accuracy'])\n",
        "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKewznIHF7B4"
      },
      "source": [
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmwYLGNcF7B8"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zEaM5kCF7B9"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, confusion_matrix\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def evaluate(X_test, Y_test, X_train, Y_train, model):\n",
        "\n",
        "    ##Evaluate loss and metrics and predict & classes\n",
        "    loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    Y_pred = model.predict(X_test, batch_size=1028)\n",
        "    Y_cls  = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "    print('Test Loss:', loss)\n",
        "    print('Accuracy: %.2f' % accuracy_score(Y_test, Y_cls))\n",
        "    print(\"Precision: %.2f\" % precision_score(Y_test, Y_cls, average='weighted'))\n",
        "    print(\"Recall: %.2f\" % recall_score(Y_test, Y_cls, average='weighted'))\n",
        "    print('Classification Report:\\n', classification_report(Y_test, Y_cls))\n",
        "\n",
        "    ## Plot 0 probability including overtraining test\n",
        "    plt.figure(figsize=(8,8))\n",
        "\n",
        "    label=1\n",
        "    #Test prediction\n",
        "    plt.hist(Y_pred[Y_test == label], alpha=0.5, color='red', range=[0, 1], bins=10)\n",
        "    plt.hist(Y_pred[Y_test != label], alpha=0.5, color='blue', range=[0, 1], bins=10)\n",
        "\n",
        "    #Train prediction\n",
        "    Y_train_pred = model.predict(X_train)\n",
        "    plt.hist(Y_train_pred[Y_train == label], alpha=0.5, color='red', range=[0, 1], bins=10, histtype='step', linewidth=2)\n",
        "    plt.hist(Y_train_pred[Y_train != label], alpha=0.5, color='blue', range=[0, 1], bins=10, histtype='step', linewidth=2)\n",
        "\n",
        "    plt.legend(['train == 1', 'train == 0', 'test == 1', 'test == 0'], loc='upper right')\n",
        "    plt.xlabel('Probability of being a good review')\n",
        "    plt.ylabel('Number of entries')\n",
        "    plt.show()\n",
        "\n",
        "    #Lets have a look at some of the incorrectly classified reviews. For readability we remove the padding.\n",
        "    predicted_classes_reshaped = np.reshape(Y_cls, Y_test.shape)\n",
        "    incorrect = np.nonzero(predicted_classes_reshaped!=Y_test)[0]\n",
        "\n",
        "    # We select the first 10 incorrectly classified reviews\n",
        "    for j, incorrect in enumerate(incorrect[0:9]):\n",
        "\n",
        "        predicted = class_names[predicted_classes_reshaped[incorrect]]\n",
        "        actual = class_names[Y_test[incorrect]]\n",
        "        human_readable_review = decode_review(X_test[incorrect])\n",
        "\n",
        "        print(\"Incorrectly classified Test Review [\"+ str(j+1) +\"]\")\n",
        "        print(\"Test Review #\" + str(incorrect)  + \": Predicted [\"+ predicted + \"] Actual [\"+ actual + \"]\")\n",
        "        print(\"Test Review Text: \" + human_readable_review.replace(\"<PAD> \", \"\"))\n",
        "        print(\"\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRsP-9v4J0qq"
      },
      "source": [
        "In order to run a bit faster, we will only evaluate the first 10000 reviews."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "186I2OHhF7CD"
      },
      "source": [
        "evaluate(X_test[:1000], y_test[:1000],X_train[:1000], y_train[:1000], model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPqkBnSCLLFb"
      },
      "source": [
        "Ok, this didn't too bad, but can we do better using a more advanced recurrent model like the LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIEe1uPUF7CH"
      },
      "source": [
        "## LSTM model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiDFDbJBF7CH"
      },
      "source": [
        "A LSTM network is an artificial neural network that contains LSTM blocks instead of, or in addition to, regular network units. A LSTM block may be described as a \"smart\" network unit that can remember a value for an arbitrary length of time.\n",
        "\n",
        "Unlike traditional RNNs, an Long short-term memory network is well-suited to learn from experience to classify, process and predict time series when there are very long time lags of unknown size between important events."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "Qm2uKrooF7CI"
      },
      "source": [
        "```python\n",
        "tensorflow.keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True,\n",
        "                            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',\n",
        "                            bias_initializer='zeros', unit_forget_bias=True, kernel_regularizer=None,\n",
        "                            recurrent_regularizer=None, bias_regularizer=None, activity_regularizer=None,\n",
        "                            kernel_constraint=None, recurrent_constraint=None, bias_constraint=None,\n",
        "                            dropout=0.0, recurrent_dropout=0.0)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-49wIBc2F7CJ"
      },
      "source": [
        "#### Arguments\n",
        "\n",
        "<ul>\n",
        "<li><strong>units</strong>: Positive integer, dimensionality of the output space.</li>\n",
        "<li><strong>activation</strong>: Activation function to use\n",
        "    If you pass None, no activation is applied\n",
        "    (ie. \"linear\" activation: <code>a(x) = x</code>).</li>\n",
        "<li><strong>recurrent_activation</strong>: Activation function to use\n",
        "    for the recurrent step.</li>\n",
        "<li><strong>use_bias</strong>: Boolean, whether the layer uses a bias vector.</li>\n",
        "<li><strong>kernel_initializer</strong>: Initializer for the <code>kernel</code> weights matrix,\n",
        "    used for the linear transformation of the inputs.</li>\n",
        "<li><strong>recurrent_initializer</strong>: Initializer for the <code>recurrent_kernel</code>\n",
        "    weights matrix,\n",
        "    used for the linear transformation of the recurrent state.</li>\n",
        "<li><strong>bias_initializer</strong>: Initializer for the bias vector.</li>\n",
        "<li><strong>unit_forget_bias</strong>: Boolean.\n",
        "    If True, add 1 to the bias of the forget gate at initialization.\n",
        "    Setting it to true will also force <code>bias_initializer=\"zeros\"</code>.\n",
        "    This is recommended in <a href=\"http://www.jmlr.org/proceedings/papers/v37/jozefowicz15.pdf\">Jozefowicz et al.</a></li>\n",
        "<li><strong>kernel_regularizer</strong>: Regularizer function applied to\n",
        "    the <code>kernel</code> weights matrix.</li>\n",
        "<li><strong>recurrent_regularizer</strong>: Regularizer function applied to\n",
        "    the <code>recurrent_kernel</code> weights matrix.</li>\n",
        "<li><strong>bias_regularizer</strong>: Regularizer function applied to the bias vector.</li>\n",
        "<li><strong>activity_regularizer</strong>: Regularizer function applied to\n",
        "    the output of the layer (its \"activation\").</li>\n",
        "<li><strong>kernel_constraint</strong>: Constraint function applied to\n",
        "    the <code>kernel</code> weights matrix.</li>\n",
        "<li><strong>recurrent_constraint</strong>: Constraint function applied to\n",
        "    the <code>recurrent_kernel</code> weights matrix.</li>\n",
        "<li><strong>bias_constraint</strong>: Constraint function applied to the bias vector.</li>\n",
        "<li><strong>dropout</strong>: Float between 0 and 1.\n",
        "    Fraction of the units to drop for\n",
        "    the linear transformation of the inputs.</li>\n",
        "<li><strong>recurrent_dropout</strong>: Float between 0 and 1.\n",
        "    Fraction of the units to drop for\n",
        "    the linear transformation of the recurrent state.</li>\n",
        "</ul>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdZLVeWvF7CJ"
      },
      "source": [
        "## Task 1: Train and evaluate an LSTM model\n",
        "* Build a model using again one embedding layer and one dense output node but with an LSTM layer with 128 units instead of the RNN layer\n",
        "* Use a dropout layer between the embedding and LSTM layer and between the LSTM and the dense layer\n",
        "* Train the model and plot the loss and accuracy over epochs\n",
        "* Evaluate the performance of the model and compare it with the RNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upN7V82PF7CK"
      },
      "source": [
        "We can now define, compile and fit our LSTM model.\n",
        "\n",
        "The first layer is the Embedded layer that uses 32 length vectors to represent each word. The next layer is the LSTM layer with 128 memory units (smart neurons). Finally, because this is a classification problem we use a Dense output layer with a single neuron and a sigmoid activation function to make 0 or 1 predictions for the two classes (good and bad) in the problem.\n",
        "\n",
        "Because it is a binary classification problem, log loss is used as the loss function (binary_crossentropy in Keras). The efficient ADAM optimization algorithm is used. The model is fit for 20 epochs. A large batch size of 128 reviews is used to space out weight updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctPJdwY1F7CL"
      },
      "source": [
        "# build a model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8rYgm9gF7CP"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGpRu2xxF7CP",
        "scrolled": false
      },
      "source": [
        "# train a model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip6wPmBIF7CY"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB3XCZZVF7CZ",
        "scrolled": false
      },
      "source": [
        "# evaluate the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW2MhuTyF7Cc"
      },
      "source": [
        "## LSTM with dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW7l-62fF7Cd"
      },
      "source": [
        "Recurrent Neural networks like LSTM generally have the problem of overfitting.\n",
        "\n",
        "Dropout can be applied between layers using the Dropout Keras layer. We have done this easily by adding new Dropout layers between the Embedding and LSTM layers and the LSTM and Dense output layers.\n",
        "\n",
        "Alternately, dropout can be applied to the input and recurrent connections of the memory units with the LSTM precisely and separately.\n",
        "Keras provides this capability with parameters on the LSTM layer, the dropout for configuring the input dropout and recurrent_dropout for configuring the recurrent dropout. For example, we can modify the first example to add dropout to the input and recurrent connections as follows:\n",
        "\n",
        "`model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQVBxN1RF7Ce"
      },
      "source": [
        "## Task 2: Train and evaluate an LSTM model with dropout\n",
        "* Instead of using two dropout layers, apply dropout to the input and recurrent connections of the LSTM model\n",
        "* Train the model over 20 epochs with a batch size of 2048 and plot the loss and accuracy over epochs\n",
        "* Evaluate the performance of the model and compare it with the previous models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQhtrsIcF7Ce"
      },
      "source": [
        "# create the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzZPdxXKF7Ci"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B89p1JqkF7Cj",
        "scrolled": false
      },
      "source": [
        "# train the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3BTEaaMF7Cp"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MmcNokDF7Cq",
        "scrolled": false
      },
      "source": [
        "# evaluate the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "696VsxWKF7Cu"
      },
      "source": [
        "## Convolutional LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZFjstuqF7Cv"
      },
      "source": [
        "Convolutional neural networks excel at learning the spatial structure in input data.\n",
        "\n",
        "The IMDB review data does have a one-dimensional spatial structure in the sequence of words in reviews and the CNN may be able to pick out invariant features for good and bad sentiment. This learned spatial features may then be learned as sequences by an LSTM layer.\n",
        "\n",
        "We can easily add a one-dimensional CNN and max pooling layers after the Embedding layer which then feed the consolidated features to the LSTM. We can use a smallish set of 32 features with a small filter length of 3. The pooling layer can use the standard length of 2 to halve the feature map size.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjjfVY5CF7Cv"
      },
      "source": [
        "## Task 3: Train and evaluate an LSTM model with a convolutional layer\n",
        "* Add one convolutional layer and one maxpooling layer before the LSTM layer\n",
        "* Train the model for 10 epochs and with a batch size of 2048 and plot the loss and accuracy over epochs\n",
        "* Evaluate the performance of the model and compare it with the previous models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fAKliBfmF7Cx"
      },
      "source": [
        "from tensorflow.keras.layers import Conv1D,MaxPooling1D\n",
        "# create the model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrWIAFrhF7C2"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sy10IIfCF7C3",
        "scrolled": false
      },
      "source": [
        "# well, you know"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELz5SnHrF7C9"
      },
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6cqb5AFUF7C-",
        "scrolled": false
      },
      "source": [
        "#"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "207qD-B2F7DB"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmPtX2KcF7DC"
      },
      "source": [
        "## Bonus: LSTM with convolutional input & recurrent transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gnju7ybF7DD"
      },
      "source": [
        "[Convolutional LSTM Network: A Machine Learning Approach for\n",
        "Precipitation Nowcasting](http://arxiv.org/abs/1506.04214v1)\n",
        "\n",
        "Based on https://github.com/keras-team/keras/blob/master/examples/conv_lstm.py\n",
        "\n",
        "This network is used to predict the next frame of an artificially generated movie which contains moving squares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYKTrSPHF7DD"
      },
      "source": [
        "#### Artificial Data Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uP-A1eeZF7DE"
      },
      "source": [
        "Generate movies with `3` to `7` moving squares inside.\n",
        "\n",
        "The squares are of shape $1 \\times 1$ or $2 \\times 2$ pixels, which move linearly over time.\n",
        "\n",
        "For convenience we first create movies with bigger width and height (`80x80`)\n",
        "and at the end we select a $40 \\times 40$ window."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qS3s_7AZF7DE"
      },
      "source": [
        "# Artificial Data Generation\n",
        "def generate_movies(n_samples=1200, n_frames=15):\n",
        "    row = 80\n",
        "    col = 80\n",
        "    noisy_movies = np.zeros((n_samples, n_frames, row, col, 1), dtype=np.float)\n",
        "    shifted_movies = np.zeros((n_samples, n_frames, row, col, 1),\n",
        "                              dtype=np.float)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Add 3 to 7 moving squares\n",
        "        n = np.random.randint(3, 8)\n",
        "\n",
        "        for j in range(n):\n",
        "            # Initial position\n",
        "            xstart = np.random.randint(20, 60)\n",
        "            ystart = np.random.randint(20, 60)\n",
        "            # Direction of motion\n",
        "            directionx = np.random.randint(0, 3) - 1\n",
        "            directiony = np.random.randint(0, 3) - 1\n",
        "\n",
        "            # Size of the square\n",
        "            w = np.random.randint(2, 4)\n",
        "\n",
        "            for t in range(n_frames):\n",
        "                x_shift = xstart + directionx * t\n",
        "                y_shift = ystart + directiony * t\n",
        "                noisy_movies[i, t, x_shift - w: x_shift + w,\n",
        "                             y_shift - w: y_shift + w, 0] += 1\n",
        "\n",
        "                # Make it more robust by adding noise.\n",
        "                # The idea is that if during inference,\n",
        "                # the value of the pixel is not exactly one,\n",
        "                # we need to train the network to be robust and still\n",
        "                # consider it as a pixel belonging to a square.\n",
        "                if np.random.randint(0, 2):\n",
        "                    noise_f = (-1)**np.random.randint(0, 2)\n",
        "                    noisy_movies[i, t,\n",
        "                                 x_shift - w - 1: x_shift + w + 1,\n",
        "                                 y_shift - w - 1: y_shift + w + 1,\n",
        "                                 0] += noise_f * 0.1\n",
        "\n",
        "                # Shift the ground truth by 1\n",
        "                x_shift = xstart + directionx * (t + 1)\n",
        "                y_shift = ystart + directiony * (t + 1)\n",
        "                shifted_movies[i, t, x_shift - w: x_shift + w,\n",
        "                               y_shift - w: y_shift + w, 0] += 1\n",
        "\n",
        "    # Cut to a 40x40 window\n",
        "    noisy_movies = noisy_movies[::, ::, 20:60, 20:60, ::]\n",
        "    shifted_movies = shifted_movies[::, ::, 20:60, 20:60, ::]\n",
        "    noisy_movies[noisy_movies >= 1] = 1\n",
        "    shifted_movies[shifted_movies >= 1] = 1\n",
        "    return noisy_movies, shifted_movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lDU9oPi9F7DI"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfA9Ra8aF7DJ"
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv3D, ConvLSTM2D, BatchNormalization\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1L2h8ilWF7DN"
      },
      "source": [
        "We create a layer which take as input movies of shape `(n_frames, width, height, channels)` and returns a movie\n",
        "of identical shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXnhMHRzF7DO"
      },
      "source": [
        "seq = Sequential()\n",
        "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
        "                   input_shape=(None, 40, 40, 1),\n",
        "                   padding='same', return_sequences=True))\n",
        "seq.add(BatchNormalization())\n",
        "\n",
        "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
        "                   padding='same', return_sequences=True))\n",
        "seq.add(BatchNormalization())\n",
        "\n",
        "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
        "                   padding='same', return_sequences=True))\n",
        "seq.add(BatchNormalization())\n",
        "\n",
        "seq.add(ConvLSTM2D(filters=40, kernel_size=(3, 3),\n",
        "                   padding='same', return_sequences=True))\n",
        "seq.add(BatchNormalization())\n",
        "\n",
        "seq.add(Conv3D(filters=1, kernel_size=(3, 3, 3),\n",
        "               activation='sigmoid',\n",
        "               padding='same', data_format='channels_last'))\n",
        "seq.compile(loss='binary_crossentropy', optimizer='adam')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AmI9-iWF7DS"
      },
      "source": [
        "### Train the Network\n",
        "\n",
        "#### Beware: This takes time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_4pFtCmF7DT"
      },
      "source": [
        "# Train the network\n",
        "noisy_movies, shifted_movies = generate_movies(n_samples=1200)\n",
        "seq.fit(noisy_movies[:1000], shifted_movies[:1000], batch_size=10,\n",
        "        epochs=20, validation_split=0.05)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZhOd1REF7DX"
      },
      "source": [
        "### Test the Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPw2EPbZF7DY"
      },
      "source": [
        "# Testing the network on one movie\n",
        "# feed it with the first 7 positions and then\n",
        "# predict the new positions\n",
        "which = 1004\n",
        "track = noisy_movies[which][:7, ::, ::, ::]\n",
        "\n",
        "for j in range(16):\n",
        "    new_pos = seq.predict(track[np.newaxis, ::, ::, ::, ::])\n",
        "    new = new_pos[::, -1, ::, ::, ::]\n",
        "    track = np.concatenate((track, new), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW-1ahZMF7Db",
        "scrolled": false
      },
      "source": [
        "# And then compare the predictions\n",
        "# to the ground truth\n",
        "track2 = noisy_movies[which][::, ::, ::, ::]\n",
        "for i in range(15):\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "\n",
        "    ax = fig.add_subplot(121)\n",
        "\n",
        "    if i >= 7:\n",
        "        ax.text(1, 3, 'Predictions !', fontsize=20, color='w')\n",
        "    else:\n",
        "        ax.text(1, 3, 'Inital trajectory', fontsize=20)\n",
        "\n",
        "    toplot = track[i, ::, ::, 0]\n",
        "\n",
        "    plt.imshow(toplot)\n",
        "    ax = fig.add_subplot(122)\n",
        "    plt.text(1, 3, 'Ground truth', fontsize=20)\n",
        "\n",
        "    toplot = track2[i, ::, ::, 0]\n",
        "    if i >= 2:\n",
        "        toplot = shifted_movies[which][i - 1, ::, ::, 0]\n",
        "\n",
        "    plt.imshow(toplot)\n",
        "    plt.savefig('%i_animate.png' % (i + 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRYot0N2QDVD"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}