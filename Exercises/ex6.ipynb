{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyekCXnrtrhb"
      },
      "source": [
        "# Exercise 6 - Hyperparameter Optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1fZQhT6trhg"
      },
      "source": [
        "In this exercise, we want to see, how to find the *optimal hyperparameters* for a simple CNN models to classify the MNIST dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh-L3U4Ytrhi"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8YnFE8ye1OJ"
      },
      "source": [
        "Load tensorflow 2:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhMr4mNuu6dC",
        "outputId": "2942d743-0d2a-499e-e950-afb3ef7decc3"
      },
      "source": [
        "#Check if colab is running\n",
        "import sys\n",
        "IN_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "  %tensorflow_version 2.x\n",
        "\n",
        "#import TF\n",
        "import tensorflow as tf\n",
        "from platform import python_version\n",
        "print(\"Tensorflow version\", tf.__version__)\n",
        "print(\"Python version =\",python_version())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version 2.5.0\n",
            "Python version = 3.7.10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFRhvIhsp0Xs"
      },
      "source": [
        "# Most of the libraries we will use in this exercise\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, InputLayer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5yyP8iltrhj"
      },
      "source": [
        "Let's first load and preprocess the data as we did in exercise 5:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vh_LOt8trhl",
        "scrolled": true
      },
      "source": [
        "np.random.seed(1338)  # for reproducibilty!!\n",
        "\n",
        "# input image dimensions\n",
        "img_rows, img_cols = 28, 28\n",
        "# number of classes\n",
        "nb_classes = 10\n",
        "\n",
        "#Load the data\n",
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mprp-IJzSP6a",
        "outputId": "698e25ef-4cf4-4c24-fa7a-c8095ec844df"
      },
      "source": [
        "num_train_samples = len(X_train)\n",
        "print(\"Number of training samples:\", num_train_samples)\n",
        "num_test_samples = len(X_test)\n",
        "print(\"Number of test samples:\", num_test_samples)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training samples: 60000\n",
            "Number of test samples: 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icbYwUOcTaex"
      },
      "source": [
        "#Take just 20k example for training for speed reasons\n",
        "X_train_reduced = X_train[:20000]\n",
        "Y_train_reduced = Y_train[:20000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJygi_BeS-PU"
      },
      "source": [
        "As the Keras scalers are not very nice in the case of images and we know (well, you might know at least), that pictures have color values between 0 and 255, we can rescale by dividing by 255. The target vector is converted to categorical."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i3R4pW9-Snon",
        "outputId": "de43391d-a6ce-484a-fbd6-66811f15ba24"
      },
      "source": [
        "print(X_train.min(), X_train.max())\n",
        "\n",
        "# scale with global maximum\n",
        "X_train_scaled = X_train_reduced/255\n",
        "X_test_scaled = X_test/255\n",
        "\n",
        "# add \"filter\" dimension (loaded images are [x,y] only, not [x,y,channels])\n",
        "X_train_ready = X_train_scaled[..., np.newaxis]\n",
        "X_test_ready = X_test_scaled[..., np.newaxis]\n",
        "\n",
        "#convert target vector\n",
        "Y_train_ready = to_categorical(Y_train_reduced, nb_classes)\n",
        "Y_test_ready = to_categorical(Y_test, nb_classes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrwyhF4otrhu"
      },
      "source": [
        "## Task 1 (recap): Model preparation\n",
        "Define a convolutional network as start point for our optimization. Design it following these points:\n",
        "- Use two convolutional layers followed by a pooling layer.\n",
        "- Use one hidden dense layer.\n",
        "- Use dropout before and after the hidden dense layer.\n",
        "- How many output nodes do you need? Add a dense output layer accordingly.\n",
        "- Which activation function should you use in the output layer?\n",
        "- Choose a loss function that is suitable for our problem at hand.\n",
        "- Compile the model and look at the summary. How many parameters do you have?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vY6vufKtrh0",
        "scrolled": true
      },
      "source": [
        "# Start here\n",
        "model = Sequential([\n",
        "                    # add layers here\n",
        "])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ncx75G6xxnzH"
      },
      "source": [
        "### A first estimate: how accurate can the model predict the digits?\n",
        "Before we start to look around for better ways to build the network, we just run it as is for now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AT16YAZphrv"
      },
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(X_train_ready, Y_train_ready, train_size=.7, test_size=.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S22Pwy3gxlWn"
      },
      "source": [
        "fit_results = model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=128,\n",
        "    epochs=15,\n",
        "    validation_data=(x_val, y_val)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWPmbwiwjroh"
      },
      "source": [
        "loss, accuracy = model.evaluate(x_train, y_train, verbose=0)\n",
        "print('Training Loss:', loss)\n",
        "print('Training Accuracy:', accuracy)\n",
        "\n",
        "val_loss, val_accuracy = model.evaluate(x_val, y_val, verbose=0)\n",
        "print('Validation Loss:', val_loss)\n",
        "print('Validation Accuracy:', val_accuracy)\n",
        "\n",
        "plt.plot(fit_results.history['loss'], label='Training')\n",
        "plt.plot(fit_results.history['val_loss'], label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.plot(fit_results.history['accuracy'], label='Training')\n",
        "plt.plot(fit_results.history['val_accuracy'], label='Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2IETbiyq1Xg"
      },
      "source": [
        "### Quick recap question:\n",
        "The accuracies returned by the model in the history object and obtained via evaluate() are very different for the training data but not for the validation data. Why is this?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gRGF0FEq8G_",
        "outputId": "b08445d6-1fdb-4b41-f1e2-05825e8d0d68"
      },
      "source": [
        "print(\"Training accuracy from history:\", fit_results.history['accuracy'][-1])\n",
        "print(\"Training accuracy with evaluate():\", accuracy)\n",
        "\n",
        "print(\"Validation accuracy from history:\", fit_results.history['val_accuracy'][-1])\n",
        "print(\"Validation accuracy with evaluate():\", val_accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training accuracy from history: 0.9687857031822205\n",
            "Training accuracy with evaluate(): 0.9957143068313599\n",
            "Validation accuracy from history: 0.9800000190734863\n",
            "Validation accuracy with evaluate(): 0.9800000190734863\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koM6AlzVtQ6-"
      },
      "source": [
        "For now, this will not be so much of a problem in this exercise as we will be mostly looking at final/ best validation accuracies, not so much at loss curves (for now). If you do not remember a possible approach to solve this discrepancy, look back in exercise 4.\n",
        "\n",
        "A callback we will definetely need though is the model checkpoint callback. With the model checkpoint callback we can store the best network during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8C1ucpwuUWm"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.models import load_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5AqUW0geCyi"
      },
      "source": [
        "# Hyperparameter optimizations: Gridsearch\n",
        "Updating parameters manually and re-running the training takes a long time and keeping track of the progress is difficult. If you want to search for an optimal hyperparameter-configuration, a grid search over a parameter space is more recommendable. The simplest form of grid search is the iteration over a parameter space."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvN6d6CnCTri"
      },
      "source": [
        "## Loop over a parameter - filter sizes\n",
        "\n",
        "As a first test, we will search for different configurations of filter sizes for our two convolutional layers. We will choose the following configuration: We set the number of filters for the first convolutional layer and set the filters of the second convolutional layer to be twice as many."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YG3FmilyeB-m"
      },
      "source": [
        "\n",
        "search_results = []\n",
        "\n",
        "filters_candidates = [\n",
        "  # very limited test, you can explore more!\n",
        "  16, 32, 64\n",
        "]\n",
        "\n",
        "for nb_filters in filters_candidates:\n",
        "  print(\"Start training for nb_filters=\", nb_filters)\n",
        "\n",
        "  ########################################\n",
        "  # Use your own model here!!\n",
        "  ########################################\n",
        "  model = Sequential([\n",
        "    Conv2D(\n",
        "        nb_filters, kernel_size=2, padding='valid',\n",
        "        activation='relu', input_shape=x_train[0].shape\n",
        "    ),\n",
        "    Conv2D(\n",
        "        int(nb_filters*2), kernel_size=2,\n",
        "        padding='valid', activation='relu'\n",
        "    ),\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Dropout(0.5),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation='softmax')\n",
        "  ])\n",
        "  model.compile(\n",
        "      loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']\n",
        "  )\n",
        "  ########################################\n",
        "\n",
        "  # we choose our best model as the one having the highest validation accuracy\n",
        "  filepath = f\"cnn_paramsearch_filters_{nb_filters}.hdf5\"\n",
        "  checkpoint = ModelCheckpoint(\n",
        "      filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max'\n",
        "  )\n",
        "\n",
        "  fit_results = model.fit(\n",
        "    x=x_train,\n",
        "    y=y_train,\n",
        "    batch_size=128,\n",
        "    # reduce number of epochs for speed reasons --> should be higher!\n",
        "    epochs=10,\n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[checkpoint],\n",
        "    verbose=0\n",
        "  )\n",
        "\n",
        "  # extract the best validation scores\n",
        "  best_val_epoch    = np.argmax(fit_results.history['val_accuracy'])\n",
        "  best_val_acc      = np.max(fit_results.history['val_accuracy'])\n",
        "  best_val_acc_loss = fit_results.history['val_loss'][best_val_epoch]\n",
        "\n",
        "  # get correct training accuracy\n",
        "  best_model = load_model(filepath)\n",
        "  best_val_acc_train_loss, best_val_acc_train_acc = best_model.evaluate(x_train, y_train)\n",
        "\n",
        "  # store results\n",
        "  search_results.append({\n",
        "      'nb_filters': nb_filters,\n",
        "      'best_val_acc_train_acc': best_val_acc_train_acc,\n",
        "      'best_val_acc': best_val_acc,\n",
        "      'best_val_acc_train_loss': best_val_acc_train_loss,\n",
        "      'best_val_acc_loss': best_val_acc_loss,\n",
        "      'best_val_epoch': best_val_epoch\n",
        "  })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aZDWyFhy1JQ"
      },
      "source": [
        "We will inspect the results using pandas DataFrames:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99msgx--y0fn"
      },
      "source": [
        "resultsDF = pd.DataFrame(search_results)\n",
        "\n",
        "# sort values\n",
        "resultsDF.sort_values('best_val_acc', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCbTL48Lhtdc"
      },
      "source": [
        "## Grid search over a more complex parameter space\n",
        "The simplest way to extend the example above to a grid containing more parameters is the following one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCRztBo03ia4"
      },
      "source": [
        "\n",
        "search_results = []\n",
        "\n",
        "filters_candidates = [\n",
        "  16, 32, 64\n",
        "]\n",
        "dense_candidates = [\n",
        "  32, 64, 128\n",
        "]\n",
        "dropout_candidates = [\n",
        "  .3, .4, .5\n",
        "]\n",
        "\n",
        "for nb_filters in filters_candidates:\n",
        "  for nb_dense in dense_candidates:\n",
        "    for dropout in dropout_candidates:\n",
        "\n",
        "      print(f\"Start training for (filters={nb_filters} - dense={nb_dense} - dropout={dropout})\")\n",
        "\n",
        "      ########################################\n",
        "      # Use your own model here!!\n",
        "      ########################################\n",
        "      model = Sequential([\n",
        "        Conv2D(\n",
        "            nb_filters, kernel_size=2, padding='valid',\n",
        "            activation='relu', input_shape=x_train[0].shape\n",
        "        ),\n",
        "        Conv2D(\n",
        "            int(nb_filters*2), kernel_size=2,\n",
        "            padding='valid', activation='relu'\n",
        "        ),\n",
        "        MaxPooling2D(pool_size=2),\n",
        "        Dropout(dropout),\n",
        "        Flatten(),\n",
        "        Dense(nb_dense, activation='relu'),\n",
        "        Dropout(dropout),\n",
        "        Dense(10, activation='softmax')\n",
        "      ])\n",
        "      model.compile(\n",
        "          loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']\n",
        "      )\n",
        "      ########################################\n",
        "\n",
        "      # we choose our best model as the one having the highest validation accuracy\n",
        "      filepath = f\"cnn_paramsearch_filters_f={nb_filters}_dn={nb_dense}_do={dropout}.hdf5\"\n",
        "      checkpoint = ModelCheckpoint(\n",
        "          filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max'\n",
        "      )\n",
        "\n",
        "      fit_results = model.fit(\n",
        "        x=x_train,\n",
        "        y=y_train,\n",
        "        batch_size=128,\n",
        "        # reduce number of epochs for speed reasons --> should be higher!\n",
        "        epochs=10,\n",
        "        validation_data=(x_val, y_val),\n",
        "        callbacks=[checkpoint],\n",
        "        verbose=0\n",
        "      )\n",
        "\n",
        "      # extract the best validation scores\n",
        "      best_val_epoch    = np.argmax(fit_results.history['val_accuracy'])\n",
        "      best_val_acc      = np.max(fit_results.history['val_accuracy'])\n",
        "      best_val_acc_loss = fit_results.history['val_loss'][best_val_epoch]\n",
        "\n",
        "      # get correct training accuracy\n",
        "      best_model = load_model(filepath)\n",
        "      best_val_acc_train_loss, best_val_acc_train_acc = best_model.evaluate(x_train, y_train)\n",
        "\n",
        "      # store results\n",
        "      search_results.append({\n",
        "          'nb_filters': nb_filters,\n",
        "          'nb_dense': nb_dense,\n",
        "          'dropout': dropout,\n",
        "          'best_val_acc_train_acc': best_val_acc_train_acc,\n",
        "          'best_val_acc': best_val_acc,\n",
        "          'best_val_acc_train_loss': best_val_acc_train_loss,\n",
        "          'best_val_acc_loss': best_val_acc_loss,\n",
        "          'best_val_epoch': best_val_epoch\n",
        "      })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rx2kM1Y40K3"
      },
      "source": [
        "resultsDF = pd.DataFrame(search_results)\n",
        "\n",
        "# sort values\n",
        "resultsDF.sort_values('best_val_acc', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHr9DroLFerG"
      },
      "source": [
        "Let's include another column that denotes the difference between train and val score:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LanHOHVvFdjq"
      },
      "source": [
        "resultsDF['delta_acc'] = (resultsDF['best_val_acc_train_acc']-resultsDF['best_val_acc'])/resultsDF['best_val_acc']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KspdcU3uDsys"
      },
      "source": [
        "A seaborn pairplot can be useful to inspect dependencies of numerical parameters on the validation loss:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbFO3xkA8Otx"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.pairplot(resultsDF, x_vars=['nb_filters', 'nb_dense', 'dropout', ], y_vars=['best_val_acc', 'best_val_acc_train_acc', 'delta_acc'], kind='reg',  height=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rR4spMpTFyGE"
      },
      "source": [
        "### Task 2: Gridsearch interpretation\n",
        "- Which parameter combination shows the best validation loss?\n",
        "- How do validation and train loss compare to each other?\n",
        "- What might be a problem choosing the hyperparameters based on the validation loss?\n",
        "- How might you further improve the robustness/ generalization performance?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs6soryti-8z"
      },
      "source": [
        "Space for answers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfWh3-2w4eIU"
      },
      "source": [
        "Testing different parameters in the way you just did is rather exhausting. We want to adjust this by using dictionaries to pass a parameter space in a more convenient way. Using `itertools.product`, we can produce all combinations from a dictionary containing lists of candidates:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFbe8WQho4jV",
        "outputId": "bbdd6772-dbe9-40e5-d1f7-0a39940634ad"
      },
      "source": [
        "# an even more reduced parameter space, but this time with structural changes\n",
        "param_space = {\n",
        "    'num_conv_layers': [1,2,3],\n",
        "    'max_filters': [32, 64],\n",
        "    'dropout': [ .4, .5],\n",
        "    'dense_nodes': [32, 64]\n",
        "}\n",
        "\n",
        "# this handy tools generates all combinations of values from the grid above\n",
        "import itertools\n",
        "value_combis = itertools.product(*[v for v in param_space.values()])\n",
        "\n",
        "param_combis = []\n",
        "for combi in value_combis:\n",
        "  param_combis.append({key: value for key, value in zip(param_space.keys(), combi)})\n",
        "\n",
        "# List comprehension short form:\n",
        "# param_combis = [{key:value for key, value in zip(param_space.keys(), combi)} for combi in value_combis]\n",
        "\n",
        "print(f\"We have a total of {len(param_combis)} combinations:\")\n",
        "print(param_combis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have a total of 24 combinations:\n",
            "[{'num_conv_layers': 1, 'max_filters': 32, 'dropout': 0.4, 'dense_nodes': 32}, {'num_conv_layers': 1, 'max_filters': 32, 'dropout': 0.4, 'dense_nodes': 64}, {'num_conv_layers': 1, 'max_filters': 32, 'dropout': 0.5, 'dense_nodes': 32}, {'num_conv_layers': 1, 'max_filters': 32, 'dropout': 0.5, 'dense_nodes': 64}, {'num_conv_layers': 1, 'max_filters': 64, 'dropout': 0.4, 'dense_nodes': 32}, {'num_conv_layers': 1, 'max_filters': 64, 'dropout': 0.4, 'dense_nodes': 64}, {'num_conv_layers': 1, 'max_filters': 64, 'dropout': 0.5, 'dense_nodes': 32}, {'num_conv_layers': 1, 'max_filters': 64, 'dropout': 0.5, 'dense_nodes': 64}, {'num_conv_layers': 2, 'max_filters': 32, 'dropout': 0.4, 'dense_nodes': 32}, {'num_conv_layers': 2, 'max_filters': 32, 'dropout': 0.4, 'dense_nodes': 64}, {'num_conv_layers': 2, 'max_filters': 32, 'dropout': 0.5, 'dense_nodes': 32}, {'num_conv_layers': 2, 'max_filters': 32, 'dropout': 0.5, 'dense_nodes': 64}, {'num_conv_layers': 2, 'max_filters': 64, 'dropout': 0.4, 'dense_nodes': 32}, {'num_conv_layers': 2, 'max_filters': 64, 'dropout': 0.4, 'dense_nodes': 64}, {'num_conv_layers': 2, 'max_filters': 64, 'dropout': 0.5, 'dense_nodes': 32}, {'num_conv_layers': 2, 'max_filters': 64, 'dropout': 0.5, 'dense_nodes': 64}, {'num_conv_layers': 3, 'max_filters': 32, 'dropout': 0.4, 'dense_nodes': 32}, {'num_conv_layers': 3, 'max_filters': 32, 'dropout': 0.4, 'dense_nodes': 64}, {'num_conv_layers': 3, 'max_filters': 32, 'dropout': 0.5, 'dense_nodes': 32}, {'num_conv_layers': 3, 'max_filters': 32, 'dropout': 0.5, 'dense_nodes': 64}, {'num_conv_layers': 3, 'max_filters': 64, 'dropout': 0.4, 'dense_nodes': 32}, {'num_conv_layers': 3, 'max_filters': 64, 'dropout': 0.4, 'dense_nodes': 64}, {'num_conv_layers': 3, 'max_filters': 64, 'dropout': 0.5, 'dense_nodes': 32}, {'num_conv_layers': 3, 'max_filters': 64, 'dropout': 0.5, 'dense_nodes': 64}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHJuMDMq94JN"
      },
      "source": [
        "With this, we can write the search a bit more condensed:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deJsI4e0133e"
      },
      "source": [
        "search_results = []\n",
        "\n",
        "for idx, params in enumerate(param_combis):\n",
        "\n",
        "    print(f\"Start run {idx+1}/{len(param_combis)}: Parameters: {params}\")\n",
        "\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(InputLayer(x_train[0].shape))\n",
        "\n",
        "    filters = params['max_filters']\n",
        "    for layer in range(params['num_conv_layers']):\n",
        "      model.add(Conv2D(\n",
        "          filters, kernel_size=2, padding='valid',\n",
        "          activation='relu'\n",
        "      ))\n",
        "      filters /= 2\n",
        "\n",
        "    for layer in [\n",
        "      MaxPooling2D(pool_size=2),\n",
        "      Dropout(params['dropout']),\n",
        "      Flatten(),\n",
        "      Dense(params['dense_nodes'], activation='relu'),\n",
        "      Dropout(params['dropout']),\n",
        "      Dense(10, activation='softmax')\n",
        "    ]:\n",
        "      model.add(layer)\n",
        "\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']\n",
        "    )\n",
        "    ########################################\n",
        "\n",
        "    # we choose our best model as the one having the highest validation accuracy\n",
        "    string_config = \"\"\n",
        "    for key, value in params.items():\n",
        "      string_config += key + \"=\" + str(value)\n",
        "    filepath = f\"cnn_paramsearch_filters_{string_config}.hdf5\"\n",
        "    checkpoint = ModelCheckpoint(\n",
        "        filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max'\n",
        "    )\n",
        "\n",
        "    fit_results = model.fit(\n",
        "      x=x_train,\n",
        "      y=y_train,\n",
        "      batch_size=128,\n",
        "      # reduce number of epochs for speed reasons --> should be higher!\n",
        "      epochs=5,\n",
        "      validation_data=(x_val, y_val),\n",
        "      callbacks=[checkpoint],\n",
        "      verbose=0\n",
        "    )\n",
        "\n",
        "    # extract the best validation scores\n",
        "    best_val_epoch    = np.argmax(fit_results.history['val_accuracy'])\n",
        "    best_val_acc      = np.max(fit_results.history['val_accuracy'])\n",
        "    best_val_acc_loss = fit_results.history['val_loss'][best_val_epoch]\n",
        "\n",
        "    # get correct training accuracy\n",
        "    best_model = load_model(filepath)\n",
        "    best_val_acc_train_loss, best_val_acc_train_acc = best_model.evaluate(x_train, y_train)\n",
        "\n",
        "    # store results\n",
        "    search_results.append({\n",
        "        **params,\n",
        "        'best_val_acc_train_acc': best_val_acc_train_acc,\n",
        "        'best_val_acc': best_val_acc,\n",
        "        'best_val_acc_train_loss': best_val_acc_train_loss,\n",
        "        'best_val_acc_loss': best_val_acc_loss,\n",
        "        'best_val_epoch': best_val_epoch\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 813
        },
        "id": "RcC_UsldANrz",
        "outputId": "6ed698bc-089b-4292-989f-af33360c92df"
      },
      "source": [
        "resultsDF = pd.DataFrame(search_results)\n",
        "\n",
        "# sort values\n",
        "resultsDF.sort_values('best_val_acc', ascending=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_conv_layers</th>\n",
              "      <th>max_filters</th>\n",
              "      <th>dropout</th>\n",
              "      <th>dense_nodes</th>\n",
              "      <th>best_val_acc_train_acc</th>\n",
              "      <th>best_val_acc</th>\n",
              "      <th>best_val_acc_train_loss</th>\n",
              "      <th>best_val_acc_loss</th>\n",
              "      <th>best_val_epoch</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.4</td>\n",
              "      <td>64</td>\n",
              "      <td>0.983286</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>0.056509</td>\n",
              "      <td>0.086908</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>0.4</td>\n",
              "      <td>64</td>\n",
              "      <td>0.981143</td>\n",
              "      <td>0.971500</td>\n",
              "      <td>0.063531</td>\n",
              "      <td>0.091963</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.5</td>\n",
              "      <td>64</td>\n",
              "      <td>0.975214</td>\n",
              "      <td>0.967000</td>\n",
              "      <td>0.082619</td>\n",
              "      <td>0.109593</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>0.5</td>\n",
              "      <td>64</td>\n",
              "      <td>0.971214</td>\n",
              "      <td>0.966833</td>\n",
              "      <td>0.096322</td>\n",
              "      <td>0.118294</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>64</td>\n",
              "      <td>0.971286</td>\n",
              "      <td>0.966167</td>\n",
              "      <td>0.094781</td>\n",
              "      <td>0.110285</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>0.4</td>\n",
              "      <td>32</td>\n",
              "      <td>0.973286</td>\n",
              "      <td>0.965667</td>\n",
              "      <td>0.088493</td>\n",
              "      <td>0.117555</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.4</td>\n",
              "      <td>32</td>\n",
              "      <td>0.976714</td>\n",
              "      <td>0.965167</td>\n",
              "      <td>0.085527</td>\n",
              "      <td>0.116529</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.5</td>\n",
              "      <td>64</td>\n",
              "      <td>0.970786</td>\n",
              "      <td>0.963167</td>\n",
              "      <td>0.104016</td>\n",
              "      <td>0.127279</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.4</td>\n",
              "      <td>64</td>\n",
              "      <td>0.974714</td>\n",
              "      <td>0.960667</td>\n",
              "      <td>0.090076</td>\n",
              "      <td>0.132021</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>2</td>\n",
              "      <td>64</td>\n",
              "      <td>0.5</td>\n",
              "      <td>32</td>\n",
              "      <td>0.968857</td>\n",
              "      <td>0.959167</td>\n",
              "      <td>0.114533</td>\n",
              "      <td>0.138049</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>3</td>\n",
              "      <td>64</td>\n",
              "      <td>0.5</td>\n",
              "      <td>32</td>\n",
              "      <td>0.964429</td>\n",
              "      <td>0.957000</td>\n",
              "      <td>0.122656</td>\n",
              "      <td>0.142708</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>0.5</td>\n",
              "      <td>64</td>\n",
              "      <td>0.961786</td>\n",
              "      <td>0.957000</td>\n",
              "      <td>0.134957</td>\n",
              "      <td>0.153567</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>64</td>\n",
              "      <td>0.965500</td>\n",
              "      <td>0.956500</td>\n",
              "      <td>0.117853</td>\n",
              "      <td>0.145588</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.5</td>\n",
              "      <td>64</td>\n",
              "      <td>0.968500</td>\n",
              "      <td>0.955833</td>\n",
              "      <td>0.116898</td>\n",
              "      <td>0.149096</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>64</td>\n",
              "      <td>0.968143</td>\n",
              "      <td>0.955500</td>\n",
              "      <td>0.114507</td>\n",
              "      <td>0.148265</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>32</td>\n",
              "      <td>0.961857</td>\n",
              "      <td>0.954833</td>\n",
              "      <td>0.134755</td>\n",
              "      <td>0.157284</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.5</td>\n",
              "      <td>64</td>\n",
              "      <td>0.961357</td>\n",
              "      <td>0.951667</td>\n",
              "      <td>0.136573</td>\n",
              "      <td>0.167762</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.4</td>\n",
              "      <td>32</td>\n",
              "      <td>0.962071</td>\n",
              "      <td>0.951500</td>\n",
              "      <td>0.134906</td>\n",
              "      <td>0.168039</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>2</td>\n",
              "      <td>32</td>\n",
              "      <td>0.5</td>\n",
              "      <td>32</td>\n",
              "      <td>0.952857</td>\n",
              "      <td>0.947667</td>\n",
              "      <td>0.175752</td>\n",
              "      <td>0.193746</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>0.5</td>\n",
              "      <td>32</td>\n",
              "      <td>0.951143</td>\n",
              "      <td>0.946667</td>\n",
              "      <td>0.176003</td>\n",
              "      <td>0.189399</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>3</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>32</td>\n",
              "      <td>0.951000</td>\n",
              "      <td>0.945167</td>\n",
              "      <td>0.164189</td>\n",
              "      <td>0.186818</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1</td>\n",
              "      <td>64</td>\n",
              "      <td>0.5</td>\n",
              "      <td>32</td>\n",
              "      <td>0.952500</td>\n",
              "      <td>0.944833</td>\n",
              "      <td>0.178655</td>\n",
              "      <td>0.202659</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.4</td>\n",
              "      <td>32</td>\n",
              "      <td>0.947143</td>\n",
              "      <td>0.938167</td>\n",
              "      <td>0.186609</td>\n",
              "      <td>0.215677</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>32</td>\n",
              "      <td>0.5</td>\n",
              "      <td>32</td>\n",
              "      <td>0.946071</td>\n",
              "      <td>0.937667</td>\n",
              "      <td>0.208931</td>\n",
              "      <td>0.236754</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    num_conv_layers  max_filters  ...  best_val_acc_loss  best_val_epoch\n",
              "13                2           64  ...           0.086908               4\n",
              "21                3           64  ...           0.091963               4\n",
              "15                2           64  ...           0.109593               4\n",
              "23                3           64  ...           0.118294               4\n",
              "17                3           32  ...           0.110285               4\n",
              "20                3           64  ...           0.117555               4\n",
              "12                2           64  ...           0.116529               4\n",
              "11                2           32  ...           0.127279               4\n",
              "5                 1           64  ...           0.132021               4\n",
              "14                2           64  ...           0.138049               4\n",
              "22                3           64  ...           0.142708               4\n",
              "19                3           32  ...           0.153567               4\n",
              "9                 2           32  ...           0.145588               4\n",
              "7                 1           64  ...           0.149096               4\n",
              "1                 1           32  ...           0.148265               4\n",
              "8                 2           32  ...           0.157284               4\n",
              "3                 1           32  ...           0.167762               4\n",
              "4                 1           64  ...           0.168039               4\n",
              "10                2           32  ...           0.193746               4\n",
              "18                3           32  ...           0.189399               4\n",
              "16                3           32  ...           0.186818               4\n",
              "6                 1           64  ...           0.202659               4\n",
              "0                 1           32  ...           0.215677               4\n",
              "2                 1           32  ...           0.236754               4\n",
              "\n",
              "[24 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TALNyfWWgdiV"
      },
      "source": [
        "## Task 3: Include loss curves\n",
        "You can also return the loss curves from your search to inspect them and search for overtraining. Include it in the search above and compare the loss curves for training and validation for the best three combinations.\n",
        "\n",
        "Bonus: We use dropout, therefore it is more correct if we use a callback function to evaluate the training loss and accuracy after the end of every epoch.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-SoEkgTiGQB"
      },
      "source": [
        "# Space for solutions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKcLfv0mBo9e"
      },
      "source": [
        "# Cross-validation - the grid searches best companion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9Kv055DtriY"
      },
      "source": [
        "The grid search we have now up and running is a really nice tool to test different hyperparameter combinations and inspect how the different parameters affect the training outcome.\n",
        "\n",
        "Using the validation scores for evaluating the generalization performance of our model we have improved compared to inspecting the training score. But especially in the case of rather small data sets, this can introduce another form of bias! If we choose a model based on the validation score, this might mean that is simply works very well for this particular validation data set out of coincidence and no real generalization was achieved. This is quite a difficult one to catch. To investigate this, one would have to test the performance on a variety of validation data sets and rather use their average score than just one result from one test. And this is, what cross-validation is about."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5D-QFrvWtrig"
      },
      "source": [
        "### basics of cross validation\n",
        "For a cross validation, the *training data* is not only divided into one training and one validation set, but different splits are performed. For a *k-fold cross validation*, the data is split into *k* parts, then the model with a certain hyperparameter configuration is trained on *k-1* of the parts and then evaluated on the remaining part. The overall score can then be computed e. g. by using the average validation score.\n",
        "\n",
        "<img src=\"https://cdn-images-1.medium.com/max/800/1*HITaUenDsDuzjAWO0zZyxg.png\" height=300>\n",
        "\n",
        "(image: https://alexforrest.github.io/you-might-be-leaking-data-even-if-you-cross-validate.html)\n",
        "\n",
        "By this, the resulting score is more likely to reflect the performance we would see on a truely unknown data set like the test data set. As the title of the source of the image suggests, there are more sophisticated ways of improving the generalization performance evaluation for our model, but cross validation certainly is a good start!\n",
        "\n",
        "`scikit-learn` provides a simple api for performing k-fold data splits similar to the `train-test-split` method."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yz31sXCifJ-"
      },
      "source": [
        "# yes, we did already import this earlier\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vmENjGWtrii"
      },
      "source": [
        "The *StratifiedKFold* preserves the relative occurences of members of different classes within the different splits. For this classification problem, we will therefore rather use this than the normal KFold.\n",
        "\n",
        "We will use cross-validation with k=3 (speed) and compute an `average precision` as a score value to find the best parameter set. The best metric for optimization depends on your problem and finding a good one for your application is a central trask of each machine learning investigation. We will modify the test function so that we will not use the TensorBoard for now:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX_BrdP5MmU_",
        "outputId": "c4deb266-58c3-4e91-f945-3fccf92474fa"
      },
      "source": [
        "# an even more reduced parameter space, but this time with structural changes\n",
        "param_space = {\n",
        "    'num_conv_layers': [2,3,4],\n",
        "    'max_filters': [32, 64],\n",
        "    'dropout': [.5],\n",
        "    'dense_nodes': [64]\n",
        "}\n",
        "\n",
        "value_combis = itertools.product(*[v for v in param_space.values()])\n",
        "\n",
        "# List comprehension short form:\n",
        "param_combis = [{key:value for key, value in zip(param_space.keys(), combi)} for combi in value_combis]\n",
        "\n",
        "print(f\"We have a total of {len(param_combis)} combinations:\")\n",
        "print(param_combis)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have a total of 6 combinations:\n",
            "[{'num_conv_layers': 2, 'max_filters': 32, 'dropout': 0.5, 'dense_nodes': 64}, {'num_conv_layers': 2, 'max_filters': 64, 'dropout': 0.5, 'dense_nodes': 64}, {'num_conv_layers': 3, 'max_filters': 32, 'dropout': 0.5, 'dense_nodes': 64}, {'num_conv_layers': 3, 'max_filters': 64, 'dropout': 0.5, 'dense_nodes': 64}, {'num_conv_layers': 4, 'max_filters': 32, 'dropout': 0.5, 'dense_nodes': 64}, {'num_conv_layers': 4, 'max_filters': 64, 'dropout': 0.5, 'dense_nodes': 64}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6ZHuKXXMyAh"
      },
      "source": [
        "def build_model_from_params(params):\n",
        "  model = Sequential()\n",
        "  model.add(InputLayer(x_train[0].shape))\n",
        "\n",
        "  filters = params['max_filters']\n",
        "  for layer in range(params['num_conv_layers']):\n",
        "    model.add(Conv2D(\n",
        "        filters, kernel_size=2, padding='valid',\n",
        "        activation='relu'\n",
        "    ))\n",
        "    filters /= 2\n",
        "\n",
        "  for layer in [\n",
        "    MaxPooling2D(pool_size=2),\n",
        "    Dropout(params['dropout']),\n",
        "    Flatten(),\n",
        "    Dense(params['dense_nodes'], activation='relu'),\n",
        "    Dropout(params['dropout']),\n",
        "    Dense(10, activation='softmax')\n",
        "  ]:\n",
        "    model.add(layer)\n",
        "\n",
        "  model.compile(\n",
        "      loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']\n",
        "  )\n",
        "  return model\n",
        "\n",
        "search_results = []\n",
        "\n",
        "k_folds = 3\n",
        "\n",
        "for idx, params in enumerate(param_combis):\n",
        "\n",
        "    print(f\"Start run {idx+1}/{len(param_combis)}: Parameters: {params}\")\n",
        "\n",
        "    k = 3\n",
        "    kf = StratifiedKFold(n_splits=k)\n",
        "\n",
        "    # we create some lists to append to during the validation folds\n",
        "    best_val_accs             = []\n",
        "    best_val_acc_losses       = []\n",
        "    best_val_acc_train_accs   = []\n",
        "    best_val_acc_train_losses = []\n",
        "\n",
        "    y_labels = np.argmax(Y_train_ready, axis=1)\n",
        "    for k_index, (train_idx, val_idx) in enumerate(kf.split(X_train_ready, y_labels)):\n",
        "      x_cv_train, x_cv_val = X_train_ready[train_idx], X_train_ready[val_idx]\n",
        "      y_cv_train, y_cv_val = Y_train_ready[train_idx], Y_train_ready[val_idx]\n",
        "\n",
        "      filepath = f\"cnn_paramsearch_filters_fold={k_index}_\"\n",
        "      for key, value in params.items():\n",
        "        filepath += key + \"=\" + str(value) + \"_\"\n",
        "\n",
        "      filepath += '.hdf5'\n",
        "      checkpoint = ModelCheckpoint(\n",
        "          filepath, monitor='val_accuracy', verbose=0, save_best_only=True, mode='max'\n",
        "      )\n",
        "\n",
        "      this_model = build_model_from_params(params)\n",
        "\n",
        "      fit_results = this_model.fit(\n",
        "        x=x_cv_train,\n",
        "        y=y_cv_train,\n",
        "        batch_size=128,\n",
        "        # reduce number of epochs for speed reasons --> should be higher!\n",
        "        epochs=5,\n",
        "        validation_data=(x_cv_val, y_cv_val),\n",
        "        callbacks=[checkpoint],\n",
        "        verbose=0\n",
        "      )\n",
        "\n",
        "      # extract the best validation scores\n",
        "      best_val_epoch = np.argmax(fit_results.history['val_accuracy'])\n",
        "      best_val_accs.append(np.max(fit_results.history['val_accuracy']))\n",
        "      best_val_acc_losses.append(fit_results.history['val_loss'][best_val_epoch])\n",
        "\n",
        "      # get correct training accuracy\n",
        "      best_model = load_model(filepath)\n",
        "      best_val_acc_train_loss, best_val_acc_train_acc = best_model.evaluate(x_train, y_train)\n",
        "      best_val_acc_train_losses.append(best_val_acc_train_loss)\n",
        "      best_val_acc_train_accs.append(best_val_acc_train_acc)\n",
        "\n",
        "    # store results\n",
        "    search_results.append({\n",
        "        **params,\n",
        "        'best_val_acc': np.mean(best_val_accs),\n",
        "        'best_val_acc_sem': np.std(best_val_accs)/np.sqrt(k),\n",
        "        'best_val_acc_train_acc': np.mean(best_val_acc_train_accs),\n",
        "        'best_val_acc_train_acc_sem': np.std(best_val_acc_train_accs)/np.sqrt(k),\n",
        "        'best_val_acc_loss': np.mean(best_val_acc_losses),\n",
        "        'best_val_acc_loss_sem': np.std(best_val_acc_losses)/np.sqrt(k),\n",
        "        'best_val_acc_train_loss': np.mean(best_val_acc_train_losses),\n",
        "        'best_val_acc_train_loss_sem': np.std(best_val_acc_train_losses)/np.sqrt(k),\n",
        "    })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwlnl5O8jVt1"
      },
      "source": [
        "resultsDF = pd.DataFrame(search_results)\n",
        "\n",
        "# sort values\n",
        "resultsDF.sort_values('best_val_acc', ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-rC-NuX8-p"
      },
      "source": [
        "You already know one way to inspect a 2D search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNaZ30DtSgMI"
      },
      "source": [
        "offset = 0\n",
        "\n",
        "# you will very likely have to tune this!!\n",
        "ylims     = (0.92, 1)\n",
        "fill_lims = (0, 0.987)\n",
        "text_vertical_loc = 0.98\n",
        "\n",
        "for num_layers, sub_df in resultsDF.groupby('num_conv_layers'):\n",
        "  sub_df_entries = len(sub_df)\n",
        "\n",
        "  x = np.arange(sub_df_entries) + offset\n",
        "\n",
        "  errb1 = plt.errorbar(x, sub_df['best_val_acc_train_acc'], yerr=sub_df['best_val_acc_train_acc_sem'], fmt='o', color='C0', label='Training')\n",
        "  errb2 = plt.errorbar(x, sub_df['best_val_acc'], yerr=sub_df['best_val_acc_sem'], fmt='o', color='C1', label='Validation')\n",
        "\n",
        "  plt.fill_betweenx(fill_lims, x[0]-0.25, x[-1]+0.25, alpha=.3)\n",
        "  plt.text((x[0]+x[-1])/2,text_vertical_loc , \"$n_\\\\mathrm{conv}$=\"+str(num_layers), horizontalalignment='center')\n",
        "  offset += sub_df_entries\n",
        "\n",
        "plt.xlabel('Max. filters')\n",
        "\n",
        "# max_filters is a list, a list times a number (number of conv layers) leads to the list being repeated!\n",
        "xlabels = param_space['max_filters']*len(param_space['num_conv_layers'])\n",
        "plt.xticks(ticks=np.arange(len(xlabels)), labels=xlabels)\n",
        "\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(ylims)\n",
        "plt.legend([errb1, errb2], ['Training', 'Validation'], loc='best')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkXmiA6striv"
      },
      "source": [
        "## Task 4: Perform a sequential grid search to optimze the following hyperparameter. Save the best model for each of the sequentiell steps into a hdf5 file.\n",
        "\n",
        "You have heard a lot about grid search and cross validation by now. Time for some final hands on!\n",
        "\n",
        "You can choose whether you want to use a cross validation or a normal grid search. What are point in favor and against?\n",
        "\n",
        "* Find the best convolution + dense structure: Change the width and the depth and try at least 4 different structures (start with dropout rate 0)\n",
        "* Find the best activation function for the dense network and the convolutional layers `'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'`\n",
        "* Find the best learning rate within [1e-4, 1e-2] (watch out, log scale!) and batch size within the range `[32, 2048]`\n",
        "* Find the best dropout rate: Change dropout between 0 and 0.5\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDwwUe6oZOpk"
      },
      "source": [
        "# space for ideas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTD6k7cctrkI"
      },
      "source": [
        "## Tips for Hyperparameter Optimization\n",
        "This section lists some handy tips to consider when tuning hyperparameters of your neural network.\n",
        "\n",
        "* **k-fold Cross Validation.** You can see that the results from the examples in this post show some variance. For speed reasons, we used a cross-validation of 2, but perhaps k=5 or k=10 would be more stable. Carefully choose your cross validation configuration to ensure your results are stable.\n",
        "* **Review the whole grid.** Do not just focus on the best result, review the whole grid of results and look for trends to support configuration decisions.\n",
        "* **Parallelize.** Use all your cores if you can, neural networks are slow to train and we often want to try a lot of different parameters. Consider using cluster instances if available.\n",
        "* **Use a subsample of your dataset.** Because networks are slow to train, try training them on a smaller sample of your training dataset, just to get an idea of general directions of parameters rather than optimal configurations.\n",
        "* **Start with coarse grids.** Start with coarse-grained grids and zoom into finer grained grids once you can narrow the scope.\n",
        "* **Do not transfer results.** Results are generally problem specific. Try to avoid favorite configurations on each new problem that you see. It is unlikely that optimal results you discover on one problem will transfer to your next project. Instead look for broader trends like number of layers or relationships between parameters.\n",
        "* **Reproducibility is a problem.** Although we set the seed for the random number generator in `NumPy`, the results are not 100% reproducible."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffLb-_xttrkJ"
      },
      "source": [
        "## Task 5: Load the best model and evaluate it using the function below\n",
        "You can load the model using<br>\n",
        "`from tensorflow.keras.models import load_model`<br>\n",
        "`model= load_model('filename')`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFFl5e4htrkK",
        "scrolled": true
      },
      "source": [
        "###################################################################################################\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_history(network_history):\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(network_history.history['loss'])\n",
        "    plt.plot(network_history.history['val_loss'])\n",
        "    plt.legend(['Training', 'Validation'])\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.plot(network_history.history['acc'])\n",
        "    plt.plot(network_history.history['val_acc'])\n",
        "    plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "    plt.show()\n",
        "\n",
        "###################################################################################################\n",
        "\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "###################################################################################################\n",
        "import matplotlib.cm as cm\n",
        "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
        "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
        "    n = 0\n",
        "    nrows = 2\n",
        "    ncols = 3\n",
        "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
        "    for row in range(nrows):\n",
        "        for col in range(ncols):\n",
        "            error = errors_index[n]\n",
        "            ax[row,col].imshow((img_errors[error]).reshape((28,28)), cmap=cm.Greys, interpolation='nearest')\n",
        "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
        "            n += 1\n",
        "\n",
        "###################################################################################################\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "\n",
        "def evaluate(X_train, Y_train, X_test, Y_test, model):\n",
        "\n",
        "    ##Evaluate loss and metrics\n",
        "    loss, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
        "    print('Test Loss:', loss)\n",
        "    print('Test Accuracy:', accuracy)\n",
        "    # Predict the values from the test dataset\n",
        "    Y_pred = model.predict(X_test)\n",
        "    # Convert predictions classes to one hot vectors\n",
        "    Y_cls = np.argmax(Y_pred, axis = 1)\n",
        "    # Convert validation observations to one hot vectors\n",
        "    Y_true = np.argmax(Y_test, axis = 1)\n",
        "    print('Classification Report:\\n', classification_report(Y_true,Y_cls))\n",
        "\n",
        "    ## Plot 0 probability including overtraining test\n",
        "    plt.figure(figsize=(8,8))\n",
        "\n",
        "    label=0\n",
        "    #Test prediction\n",
        "    Y_pred_prob = Y_pred[:,label]\n",
        "    plt.hist(Y_pred_prob[Y_true == label], alpha=0.5, color='red', range=[0, 1], bins=10, log = True)\n",
        "    plt.hist(Y_pred_prob[Y_true != label], alpha=0.5, color='blue', range=[0, 1], bins=10, log = True)\n",
        "    #Train prediction\n",
        "    Y_train_pred = model.predict(X_train)\n",
        "    Y_train_pred_prob = Y_train_pred[:,label]\n",
        "    Y_train_true = np.argmax(Y_train, axis = 1)\n",
        "    plt.hist(Y_train_pred_prob[Y_train_true == label], alpha=0.5, color='red', range=[0, 1], bins=10, log = True, histtype='step', linewidth=2)\n",
        "    plt.hist(Y_train_pred_prob[Y_train_true != label], alpha=0.5, color='blue', range=[0, 1], bins=10, log = True, histtype='step', linewidth=2)\n",
        "\n",
        "    plt.legend(['train == 0', 'train != 0', 'test == 0', 'test != 0'], loc='upper right')\n",
        "    plt.xlabel('Probability of being 0')\n",
        "    plt.ylabel('Number of entries')\n",
        "    plt.show()\n",
        "\n",
        "    # compute the confusion matrix\n",
        "    confusion_mtx = confusion_matrix(Y_true, Y_cls)\n",
        "    # plot the confusion matrix\n",
        "    plt.figure(figsize=(8,8))\n",
        "    plot_confusion_matrix(confusion_mtx, classes = range(10))\n",
        "\n",
        "    #Plot largest errors\n",
        "    errors = (Y_cls - Y_true != 0)\n",
        "    Y_cls_errors = Y_cls[errors]\n",
        "    Y_pred_errors = Y_pred[errors]\n",
        "    Y_true_errors = Y_true[errors]\n",
        "    X_test_errors = X_test[errors]\n",
        "    # Probabilities of the wrong predicted numbers\n",
        "    Y_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n",
        "    # Predicted probabilities of the true values in the error set\n",
        "    true_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n",
        "    # Difference between the probability of the predicted label and the true label\n",
        "    delta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n",
        "    # Sorted list of the delta prob errors\n",
        "    sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
        "    # Top 6 errors\n",
        "    most_important_errors = sorted_dela_errors[-6:]\n",
        "    # Show the top 6 errors\n",
        "    display_errors(most_important_errors, X_test_errors, Y_cls_errors, Y_true_errors)\n",
        "\n",
        "    ##Plot predictions\n",
        "    slice = 15\n",
        "    predicted = model.predict(X_test[:slice]).argmax(-1)\n",
        "    plt.figure(figsize=(16,8))\n",
        "    for i in range(slice):\n",
        "        plt.subplot(1, slice, i+1)\n",
        "        plt.imshow(X_test[i].reshape(28,28), interpolation='nearest')\n",
        "        plt.text(0, 0, predicted[i], color='black',\n",
        "                 bbox=dict(facecolor='white', alpha=1))\n",
        "        plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAhlo7wwtrkQ",
        "scrolled": true
      },
      "source": [
        "# space for solutions\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "jz7c5zkctrkX"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2MZR5i7trkZ"
      },
      "source": [
        "# There's more:\n",
        "\n",
        "We have had a first deep dive into cross validations and it has been quite a ride. We hope you enjoyed it though and are looking forward for more.\n",
        "\n",
        "Searching for the best hyperparameter combination is pretty much the holy grail of modern machine learning, especially deep learning. Therefore, a lot of programs and packages have been developed for smart ways to search in very high dimensional and even conditional (search this bit if this parameter is chosen, else search another bit).\n",
        "\n",
        "Please take the time to look through this nice [blog entry](https://neptune.ai/blog/hyperparameter-tuning-in-python-complete-guide#scikit-learn) where 10 of the currently most popular Python packages for hyperparameter optimization are briefly introduced.\n"
      ]
    }
  ]
}